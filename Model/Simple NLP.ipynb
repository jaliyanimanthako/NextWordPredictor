{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lZgVGhaUzkZ5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def process_text(file_path):\n",
        "    # Step 1: Read the file and store each line in a list\n",
        "    with open(file_path, \"r\", encoding=\"utf8\") as file:\n",
        "        lines = [line.strip() for line in file]\n",
        "\n",
        "    # Step 2: Combine all lines into a single string\n",
        "    data = ' '.join(lines)\n",
        "\n",
        "    # Step 3: Remove unwanted characters (\\n, \\r, \\ufeff)\n",
        "    data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "\n",
        "\n",
        "    # Step 4: Remove duplicate words and join back into a string\n",
        "    unique_words = []\n",
        "    for word in data.split():\n",
        "        if word not in unique_words:\n",
        "            unique_words.append(word)\n",
        "    data = ' '.join(unique_words)\n",
        "\n",
        "    return data\n",
        "\n",
        "file_path = \"/content/metamorphosis_clean.txt\"\n",
        "cleaned_data = process_text(file_path)\n",
        "print(cleaned_data)\n"
      ],
      "metadata": {
        "id": "pYlGclGczonu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76362fe8-ea03-4fc8-dd12-b8202b94aa49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. “What’s happened me?” thought. It wasn’t dream. room, proper human room although too small, peacefully between its four familiar walls. A collection textile samples spread out table—Samsa travelling salesman—and above there hung picture that had recently cut an illustrated magazine housed nice, gilded frame. showed lady fitted fur hat boa who sat upright, raising heavy muff covered whole her lower arm towards viewer. then turned look window at dull weather. Drops rain be heard hitting pane, which made him feel quite sad. “How I sleep bit longer forget all this nonsense”, thought, but something unable do because used sleeping right, present state couldn’t get position. However hard threw onto always rolled back where was. must have tried hundred times, shut eyes so wouldn’t floundering only stopped began mild, pain never felt before. “Oh, God”, “what strenuous career is I’ve chosen! Travelling day out. Doing business like takes much more effort than doing your own home, top there’s curse travelling, worries making train connections, bad irregular food, contact different people time you can know anyone or become friendly them. go Hell!” slight itch up belly; pushed slowly headboard lift better; was, saw lots white spots didn’t what make of; place one legs drew quickly soon touched overcome cold shudder. slid former “Getting early time”, “it makes stupid. You’ve got enough sleep. Other salesmen live life luxury. For instance, whenever guest house during morning copy contract, these gentlemen are still sitting eating their breakfasts. ought just try my boss; I’d kicked spot. But knows, maybe would best thing for me. If parents think given notice long ago, gone boss told think, tell everything would, let feel. He’d fall right desk! And it’s funny sort desk, talking down subordinates there, especially close hearing. Well, some hope; once money together pay parents’ debt him—another five six years suppose—that’s definitely I’ll do. That’s big change. First though, up, leaves five.” looked over alarm clock, ticking chest drawers. “God Heaven!” half past hands were quietly moving forwards, even later past, quarter seven. Had clock not rung? been set o’clock should been; certainly rung. Yes, possible through furniture-rattling noise? True, slept peacefully, probably deeply that. What now? next went seven; catch rush mad packed, did particularly fresh lively. avoid boss’s anger office assistant go, put report Gregor’s being ago. man, spineless, no understanding. reported sick? extremely strained suspicious service yet ill. come round doctor medical insurance company, accuse having lazy son, accept doctor’s recommendation claim believed no-one ever ill workshy. what’s more, entirely wrong case? fact, apart excessive sleepiness after long, completely well hungrier usual. hurriedly thinking through, decide bed, struck There cautious knock door near head. “Gregor”, somebody called—it mother—“it’s Didn’t want somewhere?” That gentle voice! shocked voice answering, recognised As deep inside painful uncontrollable squeaking mixed it, words first echo them unclear, leaving hearer unsure whether properly not. wanted give full answer explain everything, circumstances contented saying: “Yes, mother, yes, thank-you, I’m getting now.” change noticed outside wooden door, mother satisfied explanation shuffled away. short conversation other members family aware Gregor, against expectations father came knocking side doors, gently, fist. “Gregor, Gregor”, called, “what’s wrong?” while called again warning deepness voice: “Gregor! Gregor!” At sister plaintively: “Gregor? Aren’t well? Do need anything?” answered both sides: “I’m ready, now”, remove strangeness enunciating very carefully putting pauses each, individual word. breakfast, whispered: open beg you.” however, thought opening instead congratulated habit, acquired locking doors night home. peace without disturbed, dressed, most breakfast. Only consider next, bring thoughts sensible conclusions lying bed. remembered often perhaps caused awkwardly, pure imagination wondered how imaginings resolve themselves today. slightest doubt nothing sign serious cold, occupational hazard salesmen. simple matter throw covers; blow they fell themselves. became difficult that, exceptionally broad. arms push up; those continuously directions, moreover control. bend them, stretch itself out; finally managed leg, others free move painfully. “This can’t done bed”, said himself, “so don’t keep trying it”. part body seen part, imagine like; move; slowly; finally, almost frenzy, carelessly shoved forwards force gather, chose direction, hit bedpost, learned burning might well, present, sensitive. So first, turning side. This easily, despite breadth weight, bulk eventually followed direction last air occurred miracle injured, afraid carry pushing forward same way. now price; better stay lose consciousness. took earlier, sighing, watching struggled each harder before, possible, way bringing order chaos. whatever sacrifice. time, remind calm consideration rushing desperate conclusions. times direct clearly could, unfortunately, narrow street enveloped fog view confidence cheer offer him. “Seven o’clock, already”, again, “seven this.” longer, breathing lightly expected total stillness things real natural state. himself: “Before strikes seven will work ask me before o’clock.” task swinging entire length time. succeeded falling kept raised injuring it. hard, happen carpet. main concern loud noise bound make, raise alarm. risked. When already sticking bed—the new method game effort, rock forth—it help Two strong people—he maid mind—would enough; under dome peel away load patient careful swang floor, where, hopefully, find use. Should really call fact locked? Despite difficulty in, suppress smile After moved far across balance rocked hard. ten final decision soon. Then ring flat. “That’ll someone work”, froze still, lively danced around. moment remained quiet. “They’re door”, caught nonsensical hope. course, maid’s firm steps opened needed hear visitor’s greeting knew was—the chief clerk himself. Why condemned company immediately highly shortcoming? Were employees, every louts, faithful devoted pangs conscience spend least couple hours business? Was trainees enquiries—assuming enquiries necessary—did show whole, innocent trusted wisdom investigate it? upset decision, thump, noise. softened carpet, also elastic sound muffled noticeable. held enough, fell; annoyed pain, rubbed “Something’s fallen there”, left. today too; concede possible. gruff reply question, clerk’s footsteps polished boots adjoining room. From whispered know: here.” know”, himself; daring left, “the has wants why leave train. We say anyway, speak personally. please door. sure he’ll good forgive untidiness room.” “Good Mr. Samsa”. “He isn’t well”, clerk, continued believe else missed train! lad thinks business. nearly cross goes evenings; he’s town week stayed home evening. sits us kitchen reads paper studies timetables. idea relaxation working fretsaw. He’s frame, two three evenings, you’ll amazed nice is; hanging room; opens Anyway, glad you’re here; we ourselves; stubborn; is, isn’t.” “I’ll moment”, thoughtfully, miss word conversation. “Well explaining Mrs. Samsa”, “I hope serious. hand, commerce unwell then, fortunately unfortunately like, simply considerations.” “Can then?”, asked impatiently, again. “No”, Gregor. In silence; left cry. join others? She begun dressed. she crying? danger losing job pursue demands before? worry yet. intention abandoning family. condition seriously in. minor discourtesy, suitable excuse easily on, sacked disturbing crying. happening, worried, behaviour. voice, “Mr. wrong? You barricade yourself yes answer, causing unnecessary fail—and mention way—you fail duties unheard of. speaking here behalf employer, request clear immediate explanation. am astonished, astonished. person, suddenly seem showing peculiar whims. employer suggest reason failure appear, true—it entrusted you—but giving honour incomprehensible stubbornness wish whatsoever intercede behalf. nor position secure. originally intended private, since cause waste learn Your turnover unsatisfactory late; grant year business, recognise that; all, Samsa, cannot allow be.” “But Sir”, beside forgetting excitement, immediately, unwell, attack dizziness, haven’t up. now. now, though. Just Be patient! It’s easy alright shocking, person! night, me, small symptom already. They work! illness staying Please, suffer! There’s basis accusations making; nobody’s things. Maybe read latest contracts sent eight train, few strength. wait, sir; you, recommend him!” gushed words, knowing saying, drawers—this done, practise bed—where upright. clerk; insistent, curious sight responsibility rest. If, calmly upset, hurried station o’clock. climb smooth drawers gave swing stood upright; attention Now nearby chair tightly edges legs. By calmed down, quiet listen saying. “Did understand that?” parents, “surely fools us”. God!” tears, “he we’re suffer. Grete! Grete!” cried. “Mother?” communicated “You’ll straight Quick, doctor. Did spoke now?” “That animal”, calmness contrast mother’s screams. “Anna! Anna!” entrance hall, clapping hands, “get locksmith here, now!” girls, skirts swishing, ran wrenching front flat went. How dressed quickly? banging again; open; homes awful happened. contrast, calmer. clearer before—perhaps ears sound. realised, help. response situation confident wise, better. drawn among people, great surprising achievements—although distinguish other. Whatever crucial, so, coughed little, taking care loudly coughs judge Meanwhile, Perhaps table whispering pressed listening. chair. Once holding upright using adhesive tips rested recover involved key lock mouth. seemed, teeth—how he, grasp key?—but lack teeth jaw; jaw, start turning, ignoring kind damage fluid mouth, flowed dripped floor. “Listen”, “he’s key.” greatly encouraged this; calling too: cried, “keep hold lock!” excitedly following efforts, strength, paying around weight needed. snapped break concentration, regained breath “So, all”. handle completely. Because way, wide seen. turn double entering occupied movement, anything else, exclaim “Oh!”, sounded soughing wind. him—he nearest door—his hand mouth retreating driven steady invisible force. hair dishevelled father. unfolded arms, sank floor disappeared breast. hostile, clenched fists wanting uncertainly living wept powerful shook. leant bolted place. seen, along peered others. Meanwhile lighter; endless, grey-black building street—which hospital—could austere regular line windows piercing façade; falling, throwing large, droplets ground washing breakfast table; because, father, important meal several reading number newspapers. On wall exactly opposite photograph lieutenant army, sword carefree face forth respect uniform bearing. hall landing stairs below. “Now, then”, calm, pack off. Will leave? see”, “that stubborn job; commercial traveller arduous earn living. going, office? Yes? accurately, then? temporarily work, that’s remember achieved removed, diligence concentration. You’re our sister, trapped situation, Please already, take sides office. nobody likes travellers. enormous wage soft prejudice particular sir, overview staff, confidence, himself—it’s businessman mistakes employees harshly should. travellers office, victim gossip chance groundless complaints, impossible defend thing, usually arrive exhausted trip, harmful effects going away, partly right!” started speak, and, protruding lips, stared trembling shoulders speaking, steadily gradually, secret prohibition reached sudden foot rushed panic. stretched stairway supernatural waiting save realised question mood extreme danger. well; years, convinced provide life, besides, lost future. won over; future depended it! here! clever; tears back. lover women, surely persuade him; talk considering state, speech not—or not—be understood, door; opening; reach who, ridiculously, banister hands; scream sought onto, landed numerous Hardly than, day, body; solid them; pleasure, go; believing sorrows end. urge swayed crouched engrossed herself, jumped outstretched fingers shouting: “Help, pity’s sake, Help!” suggested better, unthinking hurrying backwards not; forgotten behind it; doing; seeming coffee pot knocked gush pouring “Mother, mother”, looking her. moment, snapping jaws flow coffee. screaming anew, fled spare now; stairs; chin banister, run reaching something, leapt disappeared; shouts resounding staircase. flight panic well. Until relatively self controlled, running impeding seized stick (the chair, overcoat), picked large newspaper drive stamping appeals help, however humbly merely stamped harder. Across chilly weather, pulled window, face. draught flew stairway, curtains newspapers fluttered blown Nothing stop drove hissing noises wild man. practice slowly. allowed impatient, threat lethal father’s Eventually, choice saw, disgust, incapable line; began, frequent anxious glances round. slowly, intentions hinder tip directions distance turn. unbearable hissing! confused. finished round, listening hissing, mistake come. pleased doorway, narrow, broad further difficulty. mood, obviously occur space through. fixed Nor preparation doorway. did, ever, way; pleasant experience, doorway regard happen. itself, angle flank scraped painfully vile flecks stuck fast quivering ground. hefty shove released flying, heavily bleeding, slammed stick, II until dark evening awoke coma-like woken afterwards anyway hadn’t fully rested. impression leading light electric lamps shone palely ceiling tops furniture, below, dark. feeling clumsily antennae—of beginning value—in happening there. one, scar, limped badly rows injured events morning—it been—and dragged lifelessly. actually smell eat. dish filled sweetened milk pieces bread floating laughed, dipped milk, covering disappointment; tender eat food—he worked snuffling whole—but taste nice. Milk normally favourite drink, turned, will, crawled centre Through crack gas lit paper, sometimes heard. write reading, habit recent times. too, though “What lead”, gazing darkness, pride such parents. wealth comfort frightening end? much, about, crawling evening, closed same; enter waited resolved either timorous visitor was; vain. previous locked everyone unlocked came, keys sides. late gaslight out, awake distinctly tip-toe. morning; plenty undisturbed re-arrange life. reason, tall, empty forced remain uneasy years. shame, couch. head, nonetheless ease regret underneath. spent Some passed sleep, frequently hunger, vague hopes which, led conclusion: patience greatest bear unpleasantness condition, impose opportunity test strength decisions, ended, anxiously couch—he somewhere, God’s flown away—she control herself outside. behaviour, tip-toe stranger. forward, edge couch, watched Would realise hunger food suitable? rather hungry draw terrible sister’s feet However, drops splashed surprise. up—using rag, bare hands—and carried place, imagining wildest possibilities, guessed goodness, bring. taste, brought selection things, old newspaper. old, half-rotten vegetables; bones meal, sauce hard; raisins almonds; cheese declared inedible days before; dry roll butter salt. poured water dish, permanently aside use, placed Then, feelings, her, comfortable liked. whirred, What’s injuries healed moving. month earlier finger knife, hurt yesterday. “Am less sensitive be, sucking greedily compellingly, attracted foods Quickly another, watering consumed cheese, vegetables sauce; foods, stand smell. Long lethargic withdraw. startled, asleep, self-control rounded breathe space. Half suffocating, bulging unselfconsciously broom swept left-overs, mixing more. dropped bin, lid, couch received second eaten midday send errand. starve either, experience feeding distress indeed suffering enough. nobody, content sighs saints later, everything—there becoming situation—that comment, comment construed friendly. “He’s enjoyed dinner today”, diligently cleared frequent, say, sadly, “now everything’s again”. Although news directly rooms, scurry appropriate press seldom conversation, secret. days, mealtime meals subject home—nobody empty. knees begged delay. within hour, tearfully thanking dismissal service. swore emphatically happened, cooking; bother ate much. unsuccessfully another eat, receive “no thanks, enough” similar. No-one drank either. beer, hoping fetch herself. add, selfish, housekeeper big, said. Even end, explained finances prospects were. receipt document cash box saved collapsed earlier. complicated taken item wanted. incarcerated different, anyway. Their misfortune reduced despair, arrange fiery vigour junior salesman representative overnight, ways. converted success benefit astonished delighted splendour, earned costs family, gratitude warm affection return. Unlike fond music gifted expressive violinist, plan conservatory expense During periods town, mentioned lovely dream realised. talk, decided planned grand announcement Christmas day. totally pointless mind tired continue listening, wearily pull start, silent. while, interrupted repeated matters explanations learned, misfortunes available days. lot, meantime interest accumulated. Besides month, keeping accumulating. Behind nodded enthusiasm pleasure unexpected thrift caution. surplus reduce boss, freed closer, money, enable interest; maintain for, perhaps, emergencies; earned. healthy lacking confidence. working—the holiday strain success—he lot slow clumsy. elderly money? suffered asthma struggling sofa window. child seventeen, till enviable, consisting wearing clothes, late, helping joining modest pleasures playing violin. Whenever cool, leather hot shame regret. lie wink scratching Or climbing sill propped leaning stare sense freedom this, experienced, distinct near; ever-present hospital street, known lived Charlottenstrasse, middle city, barren grey sky earth mingled inseparably. observant twice exact tidied inner pane on. thank easier pain. naturally, pretend burdensome unpleasant entered No sooner precaution suffer suffocating. while. shivering liked ordeal, closed. transformation appearance, usual staring motionless, horrible. coming surprise stranger threatened bite hide wait appearance flee protruded sight, carrying bedsheet arranged bent down. sheet necessary glimpsed arrangement. fourteen appreciated girl somewhat useless looked, eaten, behaved whether, improvement visit persuaded listened closely approved fully. Later, force, out: “Let unfortunate son! Can’t him?”, week, perhaps; courage, adult’s appreciation Out square meters crawl entertain walls ceiling. ceiling; floor; freely; relaxed happy, letting crash. damage. Very entertaining himself—he had, traces about—and removing furniture desk. Now, herself; dare father; sixteen bravely cook helped unless important; choose approached express joy, First, alright; enter. folds thrown chance. refrained, spying sheet; “You seen”, hand. pair feeble women heaving heaviest warnings lasted labouring fifteen minutes opposite; saddened heart; he’d abandoned quietly, (whose whereabouts know) tone added “and won’t we’ve cope himself? it’d comes unchanged easier”. Hearing communication, monotonous months, confused—he emptied transform cave, inherited? unimpeded human. forgetting, shaken removed; stay; influence condition; mindlessly loss advantage. agree; idea, spokesman concerned meant advice sufficient insist all-important childish perversity, acquired, insist; whereas see, use all. Girls age, enthusiastic can. tempted Grete shocking dominated refused dissuade had. to, writing desk stay. drawers, groaning, poked considerate but, chest, pulling without, inch. ill, end startlement, prevent little. attract attention. Grete. assure unusual admit fro, calls other, scraping assailed With longer. emptying dear containing fretsaw tools; worn homework trainee, high school, infant school—he women’s good. stepped So, catching breath, sallied changed wall—which denuded it—of copious fur. glass, firmly belly. least, no-one. watch soon; shall Her met wall. said, albeit tremor “Come let’s while?” mind, somewhere safe chase unyielding picture. jump Grete’s side, patch flowers wallpaper, screamed: “Oh God, oh Arms outstretched, immobile. “Gregor!” shouted glowering shaking spoken transformation. smelling salts faint; too—he glass force; advise days; nothing; various bottles, startled round; bottle broke; splinter face, caustic medicine delaying bottles mother; foot. death; wait; oppressed anxiety self-reproach, walls, ceiling, confusion spin table. numb immobile, quiet, sign. maid, arrived happened?” words; subdued openly chest: “Mother’s fainted, she’s out.” “Just expected”, “just listen, mean responsible act violence. delay, disappear. subtleties “Ah!”, sounding angry imagined neglected changed, father? man laying entombed trips, armchair nightgown walk Sunday public wrapped overcoat labour walking sake; invariably gather companions standing smart blue gold buttons, banking institute; high, collar coat double-chin emerged; bushy eyebrows, piercing, alert; unkempt combed scalp. cap, monogram from, probably, bank, arc sofa, trouser pockets, bottom coat, determination, walked unusually high. soles boots, wasted that—he strict stopped, scurried moved, slightly. decisive largely feared provoking step countless movements. noticeably lungs reliable. lurched efforts muster saving running; forgot although, concealed carved notches protrusions—then, tossed, apple; shock; point bombard pockets fruit bowl sideboard aim, apple another. These red apples motors. An glanced harm. Another squarely lodged back; drag surprising, incredible changing position; nailed spot senses confusion. open, screaming, blouse (as clothes fainted breathe), unfastened sliding ground, stumbling uniting totally—now ability anything—her begging III dared flesh, visible reminder injury. current sad revolting form, member treated enemy. contrary, duty swallow revulsion patient, patient. injuries, mobility—probably permanently. ancient invalid room—crawling question—but deterioration (in opinion) darkness conversation—with everyone’s permission, thus differently conversations ones longing damp hotel All nowadays. Soon dinner, chair; quiet; lamp, sew fancy underwear fashion shop; sales job, shorthand French evenings Sometimes wake “you’re sewing today!”, dozing—and exchange grin. stubbornness, home; unused peg slumber serve expecting superior here. with, result shabbier stains buttons shiny, uncomfortable peaceful. ten, gently work. obstinate table, regularly asleep importune reproaches hour refusing tug sleeve, whisper endearments ear, effect sink deeper abruptly eyes, say: life! age!” supported needle pen Who, overworked absolutely necessary? household budget smaller; dismissed; enormous, thick-boned charwoman flapped work; amount did. price hoped items jewellery belonging sold, functions celebrations. loudest complaint circumstances, imaginable transferring address. reasons move, transport crate holes unlike experienced related to. world expects poor bank sacrificed strangers, behest customers, injury new. sit together, cheek cheek; “Close Grete”, mingle, dry-eyed family’s affairs, opened; appear thoughts, apprentices, stupid teaboy, friends businesses, chambermaids provincial hotel, memory appeared cashier shop whom slow,—all strangers forgotten, inaccessible, disappeared. rage shown, wanted, plans pantry entitled hungry. midday, sweep broom, indifferent or—more not—had untouched. quicker Smears dirt balls dust filth. worst places reproach weeks touchy understood—cleaning alone. thoroughly clean bucketfuls it—although dampness bitter punished aggrieved, mothers imploring broke convulsive tears. helpless; they, agitated; accused cleaning sister; screamed anger, bedroom; quaking thumped fists; hissed closing neglected. widow, robust bone structure withstand hardest repelled curiosity, surprise, chasing fro amazement crossed failed briefly considered friendly, “come dung-beetle!”, “look dung-beetle there!” responded opened. disturb windowpanes, indicating spring coming, resentful toward infirm, attack. Instead afraid, chairs intending “Aren’t corner. eating. prepared play not, spit eating, changes anywhere rooms rented gentlemen. earnest gentlemen—all beards, peering day—were insistent things’ tidy. establishment, kitchen. Unnecessary clutter tolerate, dirty. furnishings equipment superfluous discard. dustbins too. hurry, chuck He, fortunately, object woman likely opportunity, junk else. enjoy death, immobile afterwards. everyone, lain darkest formerly, serviettes knives forks. meat piled potatoes. steaming, dishes gentleman middle, count authority two, piece wishing establish sufficiently cooked satisfaction, anxiously, smiled. Nonetheless, kitchen, bowed cap mumbled beards. alone, perfect silence. remarkable chewing heard, perform toothless be. “I’d something”, “but they’re feed am, dying!” Throughout violin played, produced newspaper, page others, smoking. attentive, hallway Someone “Is gentlemen? away.” “On contrary”, gentleman, “would young cosy comfortable?” we’d love to”, player waited. stand, begin playing; therefore exaggerated courtesy gentlemen, chairs; coat; offered seat sat—leaving it—out play; paid attention, movements hands. Drawn playing, Before, thoughtless hidden everywhere movement; threads, hairs, remains sides; wipe carpet shy immaculate preoccupied notes disturbed soon, withdrew heads sunk volume, observed anxiously. obvious beautiful disappointed, performance politeness disturbed. unnerving, blew smoke cigarettes upwards noses. Yet beautifully. lines melancholy expression. meet came. animal captivate so? shown unknown nourishment yearning for. determined skirt violin, would. lived, anyway; should, once, hiss attackers; will; ear conservatory, Christmas—had already?—if refuse hearing emotion, shoulder kiss neck, necklace collar. Samsa!”, pointing, wasting forefinger forward. silent, smiled friends, driving been. attempted block body. annoyed, behaviour dawning realisation neighbour explanations, tugged beards despair interrupted. drop bow hang limply instrument lap laboriously pressure toward. Under pillows covers beds slipped obsessed owed tenants. urged until, thunder thereby halt. declare glancing gain repugnant conditions prevail family”—here decisively floor—“I contrary proceed action damages grounds action.” silent ahead something. indeed, joined words: “And notice.” staggered seat, stretching nap uncontrolled nodding disappointment plan, weak move. “Father, Mother”, introduction, “we this. monster brother, is: rid We’ve humanly wrong.” “She’s right”, cough dully, deranged expression eyes. forehead. definite ideas. played plates occasionally “We it”, coughing “it’ll death coming. tortured endure more.” wiped mechanical “My child”, sympathy understanding, do?” shrugged helplessness displacing certainty. “If us”, question; shook vigorously question. acceptance certainty impossible, “then arrangement ...” “It’s go”, “that’s Father. harmed ourselves long. Gregor? ago beings will. brother lives respect. persecuting us, tenants, streets. Father, look, look”, screamed, starting again!” alarm, beyond comprehension, willing sacrifice excited protect anyone, sister. startling pain-wracked required deal repeatedly striking alarmed briefly. unhappy exhaustion; neck. “Maybe they’ll round”, panting ahead. separated noticing concentrated word, cry, distract neck stiff, glance asleep. shut, locked. rush. sprung lightly, “At last!”. darkness. discovery spindly unnatural. comfortable. true aching, weaker disappear altogether. decayed inflamed area dust. emotion love. strongly peaceful rumination tower strike morning. completely, weakly nostrils. cleaner morning—they’d slamming hurry she’d peace—she brief special. purpose, martyr; attributed understanding tickle nuisance resistance wide, whistled yank bedroom shout bedrooms: ’ave dead, stone dead!” marriage shock blanket shoulders, nightdress; in; paleness confirm “Dead?”, enquiringly, checked checking. “That’s said”, replied cleaner, prove sending sideways movement complete “Now “let’s thanks God that”. example. Grete, corpse, said: in”. dried flat, “Grete, while”, pained smile, wide. warmth March, breakfasts; about. “Where breakfast?”, irritably. lips quick men corpse well-worn coats. wife daughter crying little; arm. “Leave Now!”, mean?”, disconcerted, sweetly. backs continually gleeful anticipation quarrel favour. companions, contents rearranging positions. “Alright, we’ll humility permission decision. strides hallway; rubbing friend fear connection leader. hats sticks holder, premises. landing; mistrust men’s leaned progress steps. corner reappear moments later; went, butcher’s boy, proud posture tray nearer were, relieved, walk; wrote letters excusal, employers, contractor principal. writing, irritation. “Well?”, Samsa. tremendous report, vertical ostrich feather hat, source irritation directions. “Yes”, answered, laugh “well needn’t sorted intent continuing writing; describing detail prevented telling peeved, “Cheerio everyone”, sharply terribly “Tonight gets sacked”, destroyed gained. twisted then. Let’s stuff, we. Come attention”. kissed hugged letters. tram country town. tram, sunshine, Leant comfortably seats, discussed closer examination bad—until jobs promise being, house; smaller cheaper chosen location practical. livelier. cheeks pale, talking, struck, simultaneously, blossoming built lady. quieter. other’s agreed And, confirmation dreams intentions, destination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_tokenizer(cleaned_data, tokenizer_file='tokenizer1.pkl'):\n",
        "    # Tokenize the data\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts([cleaned_data])\n",
        "\n",
        "    # Save the tokenizer using pickle\n",
        "    with open(tokenizer_file, 'wb') as file:\n",
        "        pickle.dump(tokenizer, file)\n",
        "\n",
        "    # Convert text to sequences\n",
        "    sequence_data = tokenizer.texts_to_sequences([cleaned_data])[0]\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    return sequence_data  , vocab_size"
      ],
      "metadata": {
        "id": "fokOOTmN0Bix"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_data,vocab_size = save_tokenizer(cleaned_data)"
      ],
      "metadata": {
        "id": "yxvH6pkK9Sa8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "\n",
        "# Iterate over the indices of sequence_data, creating pairs of consecutive words\n",
        "for i in range(1, len(sequence_data)):\n",
        "    words = sequence_data[i-1:i+1]  # Create a pair of consecutive words\n",
        "    sequences.append(words)  # Append the pair to the sequences list\n",
        "\n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "\n",
        "sequences = np.array(sequences)  # Convert sequences to a numpy array\n",
        "sequences[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDv1pE4l0F6a",
        "outputId": "c7853bbe-e408-4b33-a2e4-cbbde7037805"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  4010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  8,  33],\n",
              "       [ 33, 272],\n",
              "       [272,  16],\n",
              "       [ 16,  34],\n",
              "       [ 34, 725],\n",
              "       [725, 108],\n",
              "       [108, 726],\n",
              "       [726, 273],\n",
              "       [273,  35],\n",
              "       [ 35, 727]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "# Iterate through each pair of consecutive words (sequences)\n",
        "for i in sequences:\n",
        "    X_train.append(i[0])  # Append the first word of the pair to X (input)\n",
        "    Y_train.append(i[1])  # Append the second word of the pair to y (output)\n",
        "\n",
        "X_train = np.array(X_train)  # Convert X to a numpy array\n",
        "Y_train = np.array(Y_train)  # Convert y to a numpy array\n"
      ],
      "metadata": {
        "id": "NoXWFuB60HqJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAC8kwVy2xFm",
        "outputId": "fe61e663-81b8-47bc-e54e-78127096b35c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 33, 272,  16,  34, 725])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert y to categorical format\n",
        "Y_train = to_categorical(Y_train, num_classes=vocab_size)\n",
        "\n",
        "# Print the first 5 elements of y after conversion\n",
        "print(Y_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99eZwVQz0Jab",
        "outputId": "6cbd3a6c-faa2-4e5d-c677-e03fd5e4cbb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer\n",
        "# vocab_size: Number of unique tokens in your vocabulary\n",
        "# 10: Dimensionality of the embedding space\n",
        "# input_length: Length of input sequences (in this case, 1 as we're predicting one word at a time)\n",
        "model.add(Embedding(vocab_size, 10, input_length=1))\n",
        "\n",
        "# Add the first LSTM layer\n",
        "# 1000: Number of units (or neurons) in the LSTM layer\n",
        "# return_sequences=True: Return the full sequence of outputs for each timestep\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "\n",
        "# Add the second LSTM layer\n",
        "# 1000: Number of units (or neurons) in the LSTM layer\n",
        "# By default, return_sequences=False, so it outputs the last output in the output sequence\n",
        "model.add(LSTM(1000))\n",
        "\n",
        "# Add a Dense layer with 1000 units and 'relu' activation function\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "\n",
        "# Add a Dense layer with vocab_size units and 'softmax' activation function\n",
        "# This layer is the output layer that predicts the probability distribution over the vocabulary\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "t4lsaEIV0LzE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI3zHgwK0NsT",
        "outputId": "5b23ede9-5718-4f58-970b-3029d1109c0c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             28000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2800)              2802800   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15879800 (60.58 MB)\n",
            "Trainable params: 15879800 (60.58 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9BIVVXK0g1a",
        "outputId": "7fe8e3a9-158f-432f-9263-3217f534c8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Assuming 'model' is already defined as per your previous code snippet\n",
        "plot_model(model, to_file='model.png', show_layer_names=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "Zetujfmv0Pug",
        "outputId": "989be0f7-81be-46cb-8213-a2ea047d71f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAIjCAYAAAAOft4aAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3deXRUZZ4+8OdWUkktUJUAIRGzQBJZEkAGmi0QmkUdV7ohgQQIW48Myzg0KhAVmuZ0N4rNknTboI0wHGfaEyqLgvQ29ihGFLARIygYEDAovxASISSQCkklfH9/0Km2zBuyVxXU8zmnziG33nrf77236uEuVfdqIiIgInKVo/N0BUTknRgORKTEcCAiJYYDESn5f3/CwYMHsXnzZk/UQkQekpOT02haoy2Hb775Brm5uW4piHxHbm4uzp8/7+ky6HvOnz/f5Oe90ZZDA1WSELWVpml48sknMX36dE+XQt+RnZ2NlJQU5XM85kBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJa8Oh+HDh8PPzw9Dhgzp0H4ff/xxdO3aFZqm4dNPP21xmz//+c+wWq3Yu3dvh9bTUp4e310OHTqEAQMGQKfTQdM0hIaG4le/+pWny0JeXh6io6OhaRo0TUNYWBjS0tI8XVan8epwOHz4MCZMmNDh/W7fvh2vvvpqq9t4+ir+nh7fXUaNGoUvvvgCDzzwAADg5MmTWL16tYerApKSknD27FnExMTAarWipKQEf/jDHzxdVqdp8mIv3kTTNE+XAAB45JFHUFFR4bPjV1dXY9KkSThw4IDHanAnX5vf7/PqLYcGer2+w/tsSeB0ZiiJCHJycrBt27ZOG6Oj7dixA6WlpZ4uw218bX6/r0PCob6+HmvWrEFkZCSMRiMGDx4Mm80GAMjMzITZbIZOp8OwYcMQGhoKvV4Ps9mMoUOHIjExERERETAYDAgKCsLKlSsb9X/69Gn0798fZrMZRqMRiYmJ+OCDD1o0PnDzg7hhwwb069cPgYGBsFqtWLFihcsYzbX54IMPEBkZCU3T8Lvf/Q4AsHXrVpjNZphMJuzZswcPPfQQLBYLwsPDkZWV5VLf888/j379+sFoNKJHjx7o06cPnn/++RZfNq2t4//2t7+FwWBAz549sWjRItx1110wGAxISEjARx99BABYunQpAgICEBYW5hzvP/7jP2A2m6FpGr799lssW7YMTz/9NM6cOQNN0xAbG9uiujvS7Ta/+/fvR1xcHKxWKwwGAwYNGoT//d//BXDzmFbDsYuYmBgUFBQAAObPnw+TyQSr1Yq33nrrlu/tX//61zCZTOjatStKS0vx9NNP4+6778bJkyfbtZyd5HtsNpsoJt/S8uXLJTAwUHJzc6W8vFyee+450el0cvjwYRER+fnPfy4A5KOPPpKqqir59ttv5cEHHxQA8qc//UnKysqkqqpKli5dKgDk008/dfY9adIkiY6Olq+++kocDod8/vnnMnLkSDEYDHLq1KkWjb9q1SrRNE02bdok5eXlYrfbZcuWLQJACgoKWtzmm2++EQDy0ksvOetbtWqVAJB33nlHKioqpLS0VBITE8VsNkttba2IiKxbt078/Pxkz549Yrfb5ciRIxIaGirjx49v1XJu6/gLFy4Us9ksJ06ckOvXr8vx48dl+PDh0rVrV/n6669FRGTWrFkSGhrqMt6GDRsEgJSVlYmISFJSksTExLSq5gYAxGazteo1//qv/yoApLy83KvmNyYmRqxWa7P15+TkyNq1a+Xy5cty6dIlGTVqlHTv3t35fFJSkvj5+cn/+3//z+V1M2fOlLfeektEWvbeBiA//elP5aWXXpKpU6fKF1980WxtDW7xec9u95bD9evXsXXrVkyZMgVJSUkICgrC6tWrodfrsXPnTpe2cXFxMJlM6N69O2bMmAEAiIyMRI8ePWAymZxHfgsLC11e17VrV/Tu3Rv+/v6Ij4/Hq6++iuvXr2Pbtm3Njl9dXY2MjAzcd999eOqppxAUFASj0Yhu3bo5+29Jm+YkJCTAYrEgJCQEqampqKqqwtdffw0A2L17N4YNG4bJkyfDaDRi6NCh+NGPfoT3338ftbW1bVrurRkfAPz9/TFgwAAEBgYiLi4OW7duxdWrVxuto9vF7TC/ycnJ+PnPf47g4GB069YNkydPxqVLl1BWVgYAWLx4Merr611qqqysxOHDh/Hwww+36rO1fv16PPHEE8jLy0P//v07pP52h8PJkydht9sxcOBA5zSj0YiwsLBGH/LvCggIAADU1dU5pzUcW3A4HLccc9CgQbBarTh27Fiz458+fRp2ux2TJk1qsr+WtGmNhnlrmI/r1683OtNQX18PvV4PPz+/DhnzVuOr/OAHP4DJZLrlOrpd3C7z2/D+rq+vBwBMnDgRffv2xX/913853x+7du1Camoq/Pz82vzZ6ijtDoeqqioAwOrVq537UJqm4dy5c7Db7e0usCl6vR4Oh6PZ8RvulRASEtJkXy1p0x4PP/wwjhw5gj179qC6uhoff/wxdu/ejUcffbRTwqGlAgMDnf+L+QJ3z++f/vQnjB8/HiEhIQgMDGx0PE3TNCxatAhnz57FO++8AwD47//+b/zbv/0bAM99thq0OxwaPlAZGRkQEZfHwYMH212gSl1dHS5fvozIyMhmxzcYDACAmpqaJvtrSZv2WLt2LSZOnIh58+bBYrFg6tSpmD59erPftehMDocDV65cQXh4uMdqcCd3ze/777+PjIwMfP3115gyZQrCwsLw0UcfoaKiAi+++GKj9vPmzYPBYMD27dtx8uRJWCwWREVFAfDMZ+u72v09h4YzDU1907Az7Nu3Dzdu3MDQoUObHX/gwIHQ6XTIz8/H4sWL29ymPY4fP44zZ86grKwM/v7e8dWS9957DyKCUaNGAbi5j97c7tztzF3ze+TIEZjNZnz22WdwOBxYsmQJoqOjAahPjQcHByMlJQW7du1C165dsWDBAudznvhsfVe7txwMBgPmz5+PrKwsbN26FZWVlaivr8f58+dx4cKFjqgRtbW1qKioQF1dHT755BMsXboUUVFRztS91fghISFITk5Gbm4uduzYgcrKShw7dszl+wUtadMeTzzxBCIjI3Ht2rUO6a8tbty4gfLyctTV1eHYsWNYtmwZIiMjMW/ePABAbGwsLl++jN27d8PhcKCsrAznzp1z6aNbt24oLi5GUVERrl696tVh4u75dTgcuHjxIt577z2YzWZERkYCAP7v//4P169fx5dffuk8lfp9ixcvRk1NDf74xz/isccec053x2frllpxaqNJNTU1kp6eLpGRkeLv7y8hISGSlJQkx48fl8zMTDGZTAJAevfuLfv375f169eL1WoVABIaGiqvv/667Nq1S0JDQwWABAcHS1ZWloiI7Ny5UyZMmCA9e/YUf39/6d69u8yYMUPOnTvXovFFRK5evSoLFiyQ7t27S5cuXWTs2LGyZs0aASDh4eFy9OjRZtssWLBAwsLCBICYTCaZPHmybNmyxTlv99xzj5w5c0a2bdsmFotFAEhUVJScOnVK3n33XenevbsAcD70er0MGDBA8vLyWrSMX3rppTaPv3DhQtHr9XL33XeLv7+/WCwW+fGPfyxnzpxx9n/p0iWZMGGCGAwG6dOnj/znf/6nrFixQgBIbGysfP311/LJJ59IVFSUGI1GGTt2rJSUlLT4PYJWnMo8dOiQxMfHi06nEwASFhYm69at8/j8vvzyyxITE+OyHlWPN954Q0RE0tPTpVu3bhIUFCTTpk2T3/3udwJAYmJinKdUG/zLv/yLPPvss42Wxa3e2y+++KIYjUYBIBEREfI///M/LV4fDW51KrNDwoFubcuWLbJs2TKXaTU1NfLkk09KYGCg2O32Th1/4cKF0q1bt04dozmtCYf28ob5ba2HH35Yzp496/ZxbxUO3rEDfAcrKSnB0qVLG+03BgQEIDIyEg6HAw6HA0ajsVPraDh95iu8fX4dDofz1OaxY8dgMBjQp08fD1fl6rb4bcXtzGg0Qq/XY8eOHbh48SIcDgeKi4uxfft2rFmzBkOGDIHVanU5VaV6pKamenpWqAOlp6fjyy+/xKlTpzB//nz88pe/9HRJjTAcOpnVasXbb7+Nzz//HH379oXRaERcXBx27tyJ9evX46OPPmp0mkr12LVrV5vGf+6557Bz505UVFSgT58+yM3N7eA59C63y/yaTCb0798f9913H9auXYu4uDhPl9SIJuL61b3s7GykpKT4zLUDyD00TYPNZmvxD83IPW7xec/hlgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJFSkxd7mTZtmjvrIB+QkZGBnJwcT5dB39FwWwaVRj/ZPnjwIDZv3tzpRZF3KSsrwxdffIFx48Z5uhTyAEVo5zQKB/JNvI4HfQ+v50BEagwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERK/p4ugNzv/PnzmDt3Lurr653Tvv32W/j7+2P8+PEubfv164ff//73bq6QvAHDwQeFh4ejqKgIZ8+ebfRcfn6+y9+JiYnuKou8DHcrfNScOXOg1+ubbZeamuqGasgbMRx81KxZs+BwOG7ZJi4uDvHx8W6qiLwNw8FHxcbGYvDgwdA0Tfm8Xq/H3Llz3VwVeROGgw+bM2cO/Pz8lM/V1dVh+vTpbq6IvAnDwYfNmDEDN27caDRd0zSMHDkSvXv3dn9R5DUYDj6sV69eSEhIgE7n+jbw8/PDnDlzPFQVeQuGg4+bPXt2o2kigqSkJA9UQ96E4eDjpk2b5rLl4Ofnh/vuuw89e/b0YFXkDRgOPi44OBgPPPCA88CkiCAtLc3DVZE3YDgQ0tLSnAcm/f39MXnyZA9XRN6A4UCYPHkyAgMDnf+2WCweroi8gdt+W5Gdne2uoagNhg4digMHDqBPnz5cV14sIiICo0ePdstYmoiIWwZq4pt4RNRyycnJyMnJccdQOW7drbDZbBARPrzoYbPZAAC1tbVYuXKlx+vho+lHcnKyOz+uPOZAN+n1eqxdu9bTZZAXYTiQk9Fo9HQJ5EUYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIyefCYfjw4fDz88OQIUM6tN/HH38cXbt2haZp+PTTT1vc5s9//jOsViv27t3bofV0lry8PERHR0PTtCYfHXG/C64nz/O5cDh8+DAmTJjQ4f1u374dr776aqvbiLjlWjsdJikpCWfPnkVMTAysVqvzWgN1dXWw2+24ePEiTCZTu8fhevI8t10mztt4y5WpHnnkEVRUVHi6jHbz8/OD0WiE0WhE3759O6xfrifP8bkthwYtuf18a7XkjdyZb3YRQU5ODrZt29ZpY7TE7t27O6wvrifP8dpwqK+vx5o1axAZGQmj0YjBgwc7L2mWmZkJs9kMnU6HYcOGITQ0FHq9HmazGUOHDkViYiIiIiJgMBgQFBSElStXNur/9OnT6N+/P8xmM4xGIxITE/HBBx+0aHzg5gresGED+vXrh8DAQFitVqxYscJljObafPDBB4iMjISmafjd734HANi6dSvMZjNMJhP27NmDhx56CBaLBeHh4cjKynKp7/nnn0e/fv1gNBrRo0cP9OnTB88//7zX3ACX6+n2WE9NEjcBIDabrcXtly9fLoGBgZKbmyvl5eXy3HPPiU6nk8OHD4uIyM9//nMBIB999JFUVVXJt99+Kw8++KAAkD/96U9SVlYmVVVVsnTpUgEgn376qbPvSZMmSXR0tHz11VficDjk888/l5EjR4rBYJBTp061aPxVq1aJpmmyadMmKS8vF7vdLlu2bBEAUlBQ0OI233zzjQCQl156yVnfqlWrBIC88847UlFRIaWlpZKYmChms1lqa2tFRGTdunXi5+cne/bsEbvdLkeOHJHQ0FAZP358q9aLzWaTtrwNYmJixGq1ukz76U9/Kp999pnLNK6njllPIiLJycmSnJzc6te1UbZXhkN1dbWYTCZJTU11TrPb7RIYGChLliwRkX++6a5eveps89prrwkAlzfo3//+dwEgu3btck6bNGmS3HvvvS5jHjt2TADI8uXLmx3fbreLyWSS+++/36WPrKws5xuqJW1Ebv2mq66udk5reLOePn1aRESGDx8uI0aMcOn73//930Wn00lNTc2tFq+L9oQDgEaPpsKB6+mf2rKeRNwfDl65W3Hy5EnY7XYMHDjQOc1oNCIsLAyFhYVNvi4gIAAAUFdX55zWsM/qcDhuOeagQYNgtVpx7NixZsc/ffo07HY7Jk2a1GR/LWnTGg3z1jAf169fb3QEvb6+Hnq93nlru8723bMVIoKf/vSnLXod15N711NbeWU4VFVVAQBWr17tcv783LlzsNvtnTauXq+Hw+Fodvzz588DAEJCQprsqyVt2uPhhx/GkSNHsGfPHlRXV+Pjjz/G7t278eijj3rsTZeZmenyQe0sXE/u4ZXh0LCiMjIyGl27/+DBg50yZl1dHS5fvozIyMhmxzcYDACAmpqaJvtrSZv2WLt2LSZOnIh58+bBYrFg6tSpmD59erPn8G93XE/u45Xh0HAEu6lvsHWGffv24caNGxg6dGiz4w8cOBA6nQ75+flN9teSNu1x/PhxnDlzBmVlZXA4HPj666+xdetWBAcHd8p4rXHhwgXMnz+/U/rmenIfrwwHg8GA+fPnIysrC1u3bkVlZSXq6+tx/vx5XLhwoUPGqK2tRUVFBerq6vDJJ59g6dKliIqKwrx585odPyQkBMnJycjNzcWOHTtQWVmJY8eOuZy3bkmb9njiiScQGRmJa9eudUh/HUFEUF1djby8vA67GS/Xkwe569AnWnkqs6amRtLT0yUyMlL8/f0lJCREkpKS5Pjx45KZmSkmk0kASO/evWX//v2yfv16sVqtAkBCQ0Pl9ddfl127dkloaKgAkODgYMnKyhIRkZ07d8qECROkZ8+e4u/vL927d5cZM2bIuXPnWjS+iMjVq1dlwYIF0r17d+nSpYuMHTtW1qxZIwAkPDxcjh492mybBQsWSFhYmAAQk8kkkydPli1btjjn7Z577pEzZ87Itm3bxGKxCACJioqSU6dOybvvvivdu3d3OVOg1+tlwIABkpeX1+Ll3NqzFW+88UaTZyq++1i9ejXXUweuJxGeyqQW2rJliyxbtsxlWk1NjTz55JMSGBgodru9Rf209VQmtUxHrScR94eDz/624nZWUlKCpUuXNtrXDggIQGRkJBwOBxwOB29v52G3+3ryymMOdGtGoxF6vR47duzAxYsX4XA4UFxcjO3bt2PNmjVITU3tsH1+arvbfT0xHG5DVqsVb7/9Nj7//HP07dsXRqMRcXFx2LlzJ9avX4/XXnvN0yUSbv/1xN2K21RiYiL+9re/eboMasbtvJ645UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTk1l9ldtaVo6ntGtZJdna2hyuh5pw/fx7h4eFuG08Tcc+9xb3lbslEt7Pk5GTk5OS4Y6gct205uCmDqI2ys7ORkpLC9UROPOZAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESv6eLoDcr6ysDG+++abLtI8//hgAsG3bNpfpXbp0wcyZM91WG3kPTUTE00WQe9XU1CAkJARVVVXw8/MDAIgIRAQ63T83Jh0OB+bMmYPXXnvNU6WS5+Rwt8IHBQYGYtq0afD394fD4YDD4UBdXR3q6+udfzscDgDgVoMPYzj4qJkzZ6K2tvaWbYKCgjBp0iQ3VUTehuHgoyZMmICQkJAmn9fr9UhLS4O/Pw9L+SqGg4/S6XSYOXMmAgIClM87HA7MmDHDzVWRN2E4+LAZM2Y0uWtx1113YfTo0W6uiLwJw8GHjRw5ElFRUY2m6/V6zJ07F5qmeaAq8hYMBx83e/Zs6PV6l2ncpSCA4eDzZs2a5Txt2SA2NhaDBw/2UEXkLRgOPq5///6Ii4tz7kLo9XrMnz/fw1WRN2A4EObMmeP8pqTD4cD06dM9XBF5A4YDITU1FfX19QCAYcOGITY21sMVkTdgOBCioqIwfPhwADe3IogA/vCqEZ6+803JycnIycnxdBneJIffjVVYtmzZHfcFoIMHDyIzMxM2m035fGVlJbZu3YpnnnnGzZV5XkZGhqdL8EoMB4XRo0ffkQflMjMzbzlfP/zhD3HPPfe4sSLvwC0GNR5zICdfDAZqGsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieHQDhs3bkTPnj2haRpeeeUVT5fTofLy8hAdHQ1N06BpGsLCwpCWlnbL1xw9ehSpqano06cPAgMD0aNHD9x777341a9+BeDm5ega+mvuMX/+fJfxf/azn91y7M2bN0PTNOh0OvTv3x/vv/9+hy0LX8VwaIfly5fjwIEDni6jUyQlJeHs2bOIiYmB1WpFSUkJ/vCHPzTZ/rPPPkNCQgLCwsKwb98+VFRU4MCBA3jwwQfx3nvvOdu9/fbbuHLlChwOBy5cuAAAmDx5Mmpra1FVVYXS0lIsWLDAZXwA2L59e6NL6Deor6/Hb3/7WwDAxIkTUVhYiHHjxnXQkvBdDAc3q66uRkJCgqfL6HAbN25EUFAQMjMz0bt3bxgMBvTt2xe//OUvYTQaAdy8BN+YMWNgtVpdbtCraRr0ej1MJhNCQkIwbNgwl76HDRuGkpIS7N69Wzl2Xl4e7r777s6bOR/FcHCzHTt2oLS01NNldLhLly6hoqICly9fdpkeEBCAvXv3AgCysrJgMpma7WvhwoV49NFHnX8vWbIEAPDyyy8r22/evBlPP/10W0unJjAcOkF+fj5GjBgBk8kEi8WCQYMGobKyEsuWLcPTTz+NM2fOQNM0xMbGIjMzE2azGTqdDsOGDUNoaCj0ej3MZjOGDh2KxMREREREwGAwICgoCCtXrvT07CkNHz4cVVVVmDhxIj788MMO7XvixIkYMGAA9u3bh5MnT7o89+GHH8Jut+OBBx7o0DGJ4dDhqqqqMHnyZCQnJ+Py5cv48ssv0bdvX9TW1iIzMxOPPfYYYmJiICI4ffo0li1bhhUrVkBE8PLLL+Orr75CSUkJxo0bh4KCAjz77LMoKCjA5cuXMXfuXGzYsAFHjx719Gw2snLlSvzgBz/A0aNHMXbsWMTHx+PXv/51oy2Jtlq0aBEANDrwu2nTJjz11FMdMga5Yjh0sKKiIlRWViI+Ph4GgwGhoaHIy8tDjx49mn1tXFwcTCYTunfv7ryRbWRkJHr06AGTyeQ8W1BYWNip89AWRqMRBw4cwG9+8xv0798fJ06cQHp6OgYMGID8/Px29z937lyYzWa89tprqK6uBgCcPXsWhw8fxsyZM9vdPzXGcOhg0dHR6NmzJ9LS0rB27VoUFRW1qZ+AgAAAQF1dnXNaw92wmzpq72l6vR5Lly7FF198gUOHDuHHP/4xSktLMW3aNJSXl7erb6vVipkzZ6K8vBy7du0CcPOS8kuWLHEuK+pYDIcOZjQa8e6772Ls2LFYt24doqOjkZqa6vzfzleMHDkSb775JhYvXoyysjLs27ev3X02HJh85ZVXcOXKFeTk5Dh3N6jjMRw6QXx8PPbu3Yvi4mKkp6fDZrNh48aNni6rw73//vvOG8IkJSW5bOU0mD17NgDAbre3e7whQ4Zg1KhR+Pvf/46FCxdi2rRpCA4Obne/pMZw6GDFxcU4ceIEACAkJAQvvPAChg4d6px2Jzly5AjMZjMAoKamRjmPDWcXBg8e3CFjNmw95Obm4sknn+yQPkmN4dDBiouLsWjRIhQWFqK2thYFBQU4d+4cRo0aBQDo1q0biouLUVRUhKtXr3rt8YNbcTgcuHjxIt577z1nOADAlClTkJ2djStXrqCiogJ79uzBM888gx/96EcdFg7Tp09Hjx49MGXKFERHR3dIn9QEIRcAxGaztajtpk2bJDQ0VACI2WyWqVOnSlFRkSQkJEhwcLD4+flJr169ZNWqVVJXVyciIp988olERUWJ0WiUsWPHyrPPPismk0kASO/evWX//v2yfv16sVqtAkBCQ0Pl9ddfl127djnHCg4OlqysrFbNl81mk9as7jfeeENiYmIEwC0fb7zxhoiIvP3225KSkiIxMTESGBgoAQEB0q9fP1m7dq1cv37dpe/KykoZN26cdOvWTQCITqeT2NhYWbdunXL8Hj16yBNPPOF8buXKlXLgwAHn36tXr5awsDBnX3FxcbJ///4Wz2tycrIkJye3uL2PyOZdtr9H0zTYbLY77l6Z2dnZSElJAVd3Y9OmTQPAe2Z+Tw53K4hIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISIlXgvoeTdM8XQJ5QHJyMq8E5SrHv/k2vsVms3m6BI84ePAgMjMzfXb+IyIiPF2C1+GWAwHgNSapEV5DkojUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFARKS6LM0AABkQSURBVEoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCRkr+nCyD3czgcuHbtmsu0qqoqAEB5ebnLdE3TEBQU5LbayHswHHzQpUuXEB4ejvr6+kbPdevWzeXv8ePHY9++fe4qjbwIdyt8UFhYGMaNGwed7tarX9M0zJgxw01VkbdhOPio2bNnQ9O0W7bR6XRISkpyU0XkbRgOPiopKQl+fn5NPu/n54cHH3wQ3bt3d2NV5E0YDj7KYrHgwQcfhL+/+rCTiCAtLc3NVZE3YTj4sLS0NOVBSQAICAjAo48+6uaKyJswHHzYY489BpPJ1Gi6v78/pkyZgi5dunigKvIWDAcfZjAYMHXqVOj1epfpdXV1mDVrloeqIm/BcPBxM2fOhMPhcJlmsVhw//33e6gi8hYMBx933333uXzxSa/XIzU1FQEBAR6sirwBw8HH+fv7IzU11blr4XA4MHPmTA9XRd6A4UCYMWOGc9ciNDQUiYmJHq6IvAHDgTBmzBj06tULwM1vTjb3tWryDT71w6vNmzfj4MGDni7DK3Xt2hUAUFBQgGnTpnm4Gu/01FNPYfTo0Z4uw2186r+IgwcP4tChQ54uwytFRkbC398fJ0+e9HQpXik3NxfffPONp8twK5/acgCAUaNGIScnx9NleKXRo0cjPDycy0ehuR+p3Yl8asuBbi08PNzTJZAXYTgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRxuYePGjejZsyc0TcMrr7zi6XKadePGDWRkZCAhIcEt4+Xl5SE6OhqapkHTNISFhTV7l6yjR48iNTUVffr0QWBgIHr06IF7770Xv/rVrwAAqampzv6ae8yfP99l/J/97Ge3HHvz5s3QNA06nQ79+/fH+++/32HL4k7EcLiF5cuX48CBA54uo0W+/PJLjBs3Dk899RTsdrtbxkxKSsLZs2cRExMDq9WKkpIS/OEPf2iy/WeffYaEhASEhYVh3759qKiowIEDB/Dggw/ivffec7Z7++23ceXKFTgcDly4cAEAMHnyZNTW1qKqqgqlpaVYsGCBy/gAsH379kaX2W9QX1+P3/72twCAiRMnorCwEOPGjeugJXFnYjh0sOrqarf9z93g6NGjeOaZZ7B48WIMGTLErWO3xsaNGxEUFITMzEz07t0bBoMBffv2xS9/+UsYjUYANy+qMmbMGFitVpf7eGqaBr1eD5PJhJCQEAwbNsyl72HDhqGkpAS7d+9Wjp2Xl4e7776782buDsRw6GA7duxAaWmpW8e89957kZeXh1mzZiEwMNCtY7fGpUuXUFFRgcuXL7tMDwgIwN69ewEAWVlZylv0fd/ChQtd7uW5ZMkSAMDLL7+sbL9582Y8/fTTbS3dJzEc2iA/Px8jRoyAyWSCxWLBoEGDUFlZiWXLluHpp5/GmTNnoGkaYmNjkZmZCbPZDJ1Oh2HDhiE0NBR6vR5msxlDhw5FYmIiIiIiYDAYEBQUhJUrV3p69jrN8OHDUVVVhYkTJ+LDDz/s0L4nTpyIAQMGYN++fY2ug/nhhx/CbrfjgQce6NAx73QMh1aqqqrC5MmTkZycjMuXL+PLL79E3759UVtbi8zMTDz22GOIiYmBiOD06dNYtmwZVqxYARHByy+/jK+++golJSUYN24cCgoK8Oyzz6KgoACXL1/G3LlzsWHDBhw9etTTs9kpVq5ciR/84Ac4evQoxo4di/j4ePz6179utCXRVosWLQKARgePN23ahKeeeqpDxvAlDIdWKioqQmVlJeLj42EwGBAaGoq8vDz06NGj2dfGxcXBZDKhe/fumDFjBoCbV33u0aMHTCaT80h/YWFhp86DpxiNRhw4cAC/+c1v0L9/f5w4cQLp6ekYMGAA8vPz293/3LlzYTab8dprr6G6uhoAcPbsWRw+fJh38WoDhkMrRUdHo2fPnkhLS8PatWtRVFTUpn4a7kVZV1fnnPbdW9LdqfR6PZYuXYovvvgChw4dwo9//GOUlpZi2rRpKC8vb1ffVqsVM2fORHl5OXbt2gUAyMjIwJIlS3jvzzZgOLSS0WjEu+++i7Fjx2LdunWIjo5Gamqq838qarmRI0fizTffxOLFi1FWVoZ9+/a1u8+GA5OvvPIKrly5gpycHOfuBrUOw6EN4uPjsXfvXhQXFyM9PR02mw0bN270dFle6f3330dGRgaAm9+L+O6WUoPZs2cDQId8P2PIkCEYNWoU/v73v2PhwoWYNm0agoOD292vL2I4tFJxcTFOnDgBAAgJCcELL7yAoUOHOqeRqyNHjsBsNgMAampqlMup4ezC4MGDO2TMhq2H3NxcPPnkkx3Spy9iOLRScXExFi1ahMLCQtTW1qKgoADnzp3DqFGjAADdunVDcXExioqKcPXq1Tv6+MGtOBwOXLx4Ee+9954zHABgypQpyM7OxpUrV1BRUYE9e/bgmWeewY9+9KMOC4fp06ejR48emDJlCqKjozukT58kPiQ5OVmSk5Nb3H7Tpk0SGhoqAMRsNsvUqVOlqKhIEhISJDg4WPz8/KRXr16yatUqqaurExGRTz75RKKiosRoNMrYsWPl2WefFZPJJACkd+/esn//flm/fr1YrVYBIKGhofL666/Lrl27nGMFBwdLVlZWi+s8ePCgjBkzRu666y4BIAAkLCxMEhISJD8/v9OWzxtvvCExMTHOMZt6vPHGGyIi8vbbb0tKSorExMRIYGCgBAQESL9+/WTt2rVy/fp1l74rKytl3Lhx0q1bNwEgOp1OYmNjZd26dcrxe/ToIU888YTzuZUrV8qBAwecf69evVrCwsKcfcXFxcn+/ftbPK8AxGaztbj9HSBbExFxdyB5SsPdo3kvSDUun6ZpmgabzYbp06d7uhR3yeFuBREpMRy8VGFhYYt+tpyamurpUukO5d98E/KE/v37w4f2+MgLccuBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREo+95PtQ4cOOa94RK4OHToEAFw+BMDHwmH06NGeLsFrlZWVoba2lrelb0JycjIiIiI8XYZb+dQ1JKlp2dnZSElJ4QVmqAGvIUlEagwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERK/p4ugNzv/PnzmDt3Lurr653Tvv32W/j7+2P8+PEubfv164ff//73bq6QvAHDwQeFh4ejqKgIZ8+ebfRcfn6+y9+JiYnuKou8DHcrfNScOXOg1+ubbZeamuqGasgbMRx81KxZs+BwOG7ZJi4uDvHx8W6qiLwNw8FHxcbGYvDgwdA0Tfm8Xq/H3Llz3VwVeROGgw+bM2cO/Pz8lM/V1dVh+vTpbq6IvAnDwYfNmDEDN27caDRd0zSMHDkSvXv3dn9R5DUYDj6sV69eSEhIgE7n+jbw8/PDnDlzPFQVeQuGg4+bPXt2o2kigqSkJA9UQ96E4eDjpk2b5rLl4Ofnh/vuuw89e/b0YFXkDRgOPi44OBgPPPCA88CkiCAtLc3DVZE3YDgQ0tLSnAcm/f39MXnyZA9XRN6A4UCYPHkyAgMDnf+2WCweroi8AX9b8Q/Z2dmeLsGjhg4digMHDqBPnz4+vSwiIiIwevRoT5fhFTQREU8X4Q2a+qYg+Zbk5GTk5OR4ugxvkMPdiu+w2WwQEZ962Gw2AEBtbS1Wrlzp8Xo8+UhOTvbwO9C7MBwIwM3fUqxdu9bTZZAXYTiQk9Fo9HQJ5EUYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieHQQR5//HF07doVmqbh008/9XQ5nSYvLw/R0dHQNM3lERAQgJ49e2L8+PHYsGEDysvLPV0qtRPDoYNs374dr776qqfL6HRJSUk4e/YsYmJiYLVaISK4ceMGSktLkZ2djT59+iA9PR3x8fH4+OOPPV0utQPDgdpN0zQEBQVh/Pjx2LlzJ7Kzs3Hx4kU88sgjqKio8HR51EYMhw7ES83dlJycjHnz5qG0tBSvvPKKp8uhNmI4tJGIYMOGDejXrx8CAwNhtVqxYsUKlzb19fVYs2YNIiMjYTQaMXjwYOdl2bZu3Qqz2QyTyYQ9e/bgoYcegsViQXh4OLKyspx95OfnY8SIETCZTLBYLBg0aBAqKyub7d/T5s2bBwD4y1/+AsC3l8VtS0hERACIzWZrcftVq1aJpmmyadMmKS8vF7vdLlu2bBEAUlBQICIiy5cvl8DAQMnNzZXy8nJ57rnnRKfTyeHDh519AJB33nlHKioqpLS0VBITE8VsNkttba1cu3ZNLBaLvPjii1JdXS0lJSUydepUKSsra1H/LWGz2aQtb4OYmBixWq1NPl9ZWSkAJCIi4rZZFsnJyZKcnNzqZXGHymY4/ENrwsFut4vJZJL777/fZXpWVpYzHKqrq8VkMklqaqrL6wIDA2XJkiUi8s8PRHV1tbNNQ8CcPn1aPv/8cwEgf/zjHxvV0JL+W6KzwkFERNM0CQoKum2WBcPBRTZ3K9rg9OnTsNvtmDRpUpNtTp48CbvdjoEDBzqnGY1GhIWFobCwsMnXBQQEAAAcDgeio6PRs2dPpKWlYe3atSgqKmp3/+5SVVUFEYHFYvH5ZXG7Yji0wfnz5wEAISEhTbapqqoCAKxevdrl+wDnzp2D3W5v0ThGoxHvvvsuxo4di3Xr1iE6Ohqpqamorq7ukP4706lTpwAA/fv39/llcbtiOLSBwWAAANTU1DTZpiE4MjIyGt0f4eDBgy0eKz4+Hnv37kVxcTHS09Nhs9mwcePGDuu/s/z1r38FADz00EM+vyxuVwyHNhg4cCB0Oh3y8/ObbBMREQGDwdCub0sWFxfjxIkTAG6GzQsvvIChQ4fixIkTHdJ/ZykpKUFGRgbCw8Pxk5/8xKeXxe2M4dAGISEhSE5ORm5uLnbs2IHKykocO3YM27Ztc7YxGAyYP38+srKysHXrVlRWVqK+vh7nz5/HhQsXWjROcXExFi1ahMLCQtTW1qKgoADnzp3DqFGjOqT/9hIRXLt2DTdu3ICIoKysDDabDWPGjIGfnx92794Ni8XiE8vijuTmI6BeC608lXn16lVZsGCBdO/eXbp06SJjx46VNWvWCAAJDw+Xo0ePSk1NjaSnp0tkZKT4+/tLSEiIJCUlyfHjx2XLli1iMpkEgNxzzz1y5swZ2bZtm1gsFgEgUVFR8re//U0SEhIkODhY/Pz8pFevXrJq1Sqpq6sTEbll/y3V2rMVb731lgwePFhMJpMEBASITqcTAM4zEyNGjJBf/OIXcunSJZfX3Q7LgmcrXGTzRrr/oGkabDYbpk+f7ulS3Co7OxspKSng2wCYNm0aAPBGujfxRrpEpMZwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTk7+kCvIkvXqm4YZ6zs7M9XInnnT9/HuHh4Z4uw2vwMnH/wJvgEnDzJsC8TBwAIIdbDv/g6xnJa0nS9/GYAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEciEiJ4UBESgwHIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESn5e7oAcr+ysjK8+eabLtM+/vhjAMC2bdtcpnfp0gUzZ850W23kPTQREU8XQe5VU1ODkJAQVFVVwc/PDwAgIhAR6HT/3Jh0OByYM2cOXnvtNU+VSp6Tw90KHxQYGIhp06bB398fDocDDocDdXV1qK+vd/7tcDgAgFsNPozh4KNmzpyJ2traW7YJCgrCpEmT3FQReRuGg4+aMGECQkJCmnxer9cjLS0N/v48LOWrGA4+SqfTYebMmQgICFA+73A4MGPGDDdXRd6E4eDDZsyY0eSuxV133YXRo0e7uSLyJgwHHzZy5EhERUU1mq7X6zF37lxomuaBqshbMBx83OzZs6HX612mcZeCAIaDz5s1a5bztGWD2NhYDB482EMVkbdgOPi4/v37Iy4uzrkLodfrMX/+fA9XRd6A4UCYM2eO85uSDocD06dP93BF5A0YDoTU1FTU19cDAIYNG4bY2FgPV0TegOFAiIqKwvDhwwHc3IogAnzgh1fZ2dlISUnxdBl0h7nDPzYAkOMz34212WyeLsGrVVZWYuvWrXjmmWeUz6ekpGDZsmU+/8WogwcPIjMz09NluIXPhAMPsjXvhz/8Ie655x7lcykpKRg9ejSXI+Az4cBjDuTUVDCQb2I4EJESw4GIlBgORKTEcCAiJYYDESkxHIhIieFAREoMByJSYjgQkRLDgYiUGA5EpMRwICIlhgMRKTEcWuDxxx9H165doWkaPv30U0+X02o3btxARkYGEhIS3DJeXl4eoqOjoWmayyMgIAA9e/bE+PHjsWHDBpSXl7ulHmobhkMLbN++Ha+++qqny2iTL7/8EuPGjcNTTz0Fu93uljGTkpJw9uxZxMTEwGq1QkRw48YNlJaWIjs7G3369EF6ejri4+Px8ccfu6Umaj2Gwx3s6NGjeOaZZ7B48WIMGTLEo7VomoagoCCMHz8eO3fuRHZ2Ni5evIhHHnkEFRUVHq2N1BgOLXQ73hru3nvvRV5eHmbNmoXAwEBPl+MiOTkZ8+bNQ2lpKV555RVPl0MKDAcFEcGGDRvQr18/BAYGwmq1YsWKFS5t6uvrsWbNGkRGRsJoNGLw4MHO61Ru3boVZrMZJpMJe/bswUMPPQSLxYLw8HBkZWU5+8jPz8eIESNgMplgsVgwaNAgVFZWNtv/nWLevHkAgL/85S8AuEy9jtzhbDabtHY2V61aJZqmyaZNm6S8vFzsdrts2bJFAEhBQYGIiCxfvlwCAwMlNzdXysvL5bnnnhOdTieHDx929gFA3nnnHamoqJDS0lJJTEwUs9kstbW1cu3aNbFYLPLiiy9KdXW1lJSUyNSpU6WsrKxF/bfWyJEj5d57723Ta0VEAIjNZmvVa2JiYsRqtTb5fGVlpQCQiIgIEbk9lmlb3k+3qew7fi5buzLtdruYTCa5//77XaZnZWU5w6G6ulpMJpOkpqa6vC4wMFCWLFkiIv98I1dXVzvbNATM6dOn5fPPPxcA8sc//rFRDS3pv7W8MRxERDRNk6CgoNtmmfpSOHC34ntOnz4Nu92OSZMmNdnm5MmTsNvtGDhwoHOa0WhEWFgYCgsLm3xdQEAAgJu3nIuOjkbPnj2RlpaGtWvXoqioqN39326qqqogIrBYLFymXojh8D3nz58HAISEhDTZpqqqCgCwevVql/P4586da/HpQqPRiHfffRdjx47FunXrEB0djdTUVFRXV3dI/7eDU6dOAbh5M18uU+/DcPgeg8EAAKipqWmyTUNwZGRkQERcHgcPHmzxWPHx8di7dy+Ki4uRnp4Om82GjRs3dlj/3u6vf/0rAOChhx7iMvVCDIfvGThwIHQ6HfLz85tsExERAYPB0K5vSxYXF+PEiRMAbobNCy+8gKFDh+LEiRMd0r+3KykpQUZGBsLDw/GTn/yEy9QLMRy+JyQkBMnJycjNzcWOHTtQWVmJY8eOYdu2bc42BoMB8+fPR1ZWFrZu3YrKykrU19fj/PnzuHDhQovGKS4uxqJFi1BYWIja2loUFBTg3LlzGDVqVIf07y1EBNeuXcONGzcgIigrK4PNZsOYMWPg5+eH3bt3w2KxcJl6IzcfAXW7thxdvnr1qixYsEC6d+8uXbp0kbFjx8qaNWsEgISHh8vRo0elpqZG0tPTJTIyUvz9/SUkJESSkpLk+PHjsmXLFjGZTAJA7rnnHjlz5oxs27ZNLBaLAJCoqCj529/+JgkJCRIcHCx+fn7Sq1cvWbVqldTV1YmI3LL/ljp48KCMGTNG7rrrLgEgACQsLEwSEhIkPz+/VcsErThb8dZbb8ngwYPFZDJJQECA6HQ6AeA8MzFixAj5xS9+IZcuXXJ53e2wTH3pbIXP3GX7Dp/NTqdpGmw2m8/fK9OH3k853K0gIiWGw22msLCw0U+hVY/U1FRPl0q3OX9PF0Ct079/f1/YpCUvwC0HIlJiOBCREsOBiJQYDkSkxHAgIiWGAxEpMRyISInhQERKDAciUmI4EJESw4GIlBgORKTEcCAiJYYDESn5zE+2b8d7XXqblJQUpKSkeLoMcpM7PhwSEhJ4P0SiNrjjryFJRG3Ca0gSkRrDgYiUGA5EpOQPIMfTRRCR1zn0/wFwfejZSEyxaQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto')\n",
        "\n",
        "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
        "\n",
        "logdir='logsnextword1'\n",
        "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
      ],
      "metadata": {
        "id": "hZZRpVwo0SjT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrGSXcYH0rf9",
        "outputId": "ffb5b9a9-6a92-48c2-946d-b86d9e52d0c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk0miB6N0tyh",
        "outputId": "413861cc-c1fc-4ab0-b917-9202accf4c7c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.9435\n",
            "Epoch 1: loss improved from inf to 7.94354, saving model to nextword1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r63/63 [==============================] - 25s 323ms/step - loss: 7.9435 - lr: 0.0010\n",
            "Epoch 2/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.9235\n",
            "Epoch 2: loss improved from 7.94354 to 7.92354, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 312ms/step - loss: 7.9235 - lr: 0.0010\n",
            "Epoch 3/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.8847\n",
            "Epoch 3: loss improved from 7.92354 to 7.88472, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 297ms/step - loss: 7.8847 - lr: 0.0010\n",
            "Epoch 4/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.6992\n",
            "Epoch 4: loss improved from 7.88472 to 7.69918, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 316ms/step - loss: 7.6992 - lr: 0.0010\n",
            "Epoch 5/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.5035\n",
            "Epoch 5: loss improved from 7.69918 to 7.50355, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 303ms/step - loss: 7.5035 - lr: 0.0010\n",
            "Epoch 6/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.3312\n",
            "Epoch 6: loss improved from 7.50355 to 7.33117, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 302ms/step - loss: 7.3312 - lr: 0.0010\n",
            "Epoch 7/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.2120\n",
            "Epoch 7: loss improved from 7.33117 to 7.21202, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 286ms/step - loss: 7.2120 - lr: 0.0010\n",
            "Epoch 8/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.1361\n",
            "Epoch 8: loss improved from 7.21202 to 7.13610, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 7.1361 - lr: 0.0010\n",
            "Epoch 9/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 7.0577\n",
            "Epoch 9: loss improved from 7.13610 to 7.05768, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 304ms/step - loss: 7.0577 - lr: 0.0010\n",
            "Epoch 10/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.9786\n",
            "Epoch 10: loss improved from 7.05768 to 6.97856, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 289ms/step - loss: 6.9786 - lr: 0.0010\n",
            "Epoch 11/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.9097\n",
            "Epoch 11: loss improved from 6.97856 to 6.90971, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 294ms/step - loss: 6.9097 - lr: 0.0010\n",
            "Epoch 12/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.8297\n",
            "Epoch 12: loss improved from 6.90971 to 6.82967, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 313ms/step - loss: 6.8297 - lr: 0.0010\n",
            "Epoch 13/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.7713\n",
            "Epoch 13: loss improved from 6.82967 to 6.77135, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 296ms/step - loss: 6.7713 - lr: 0.0010\n",
            "Epoch 14/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.7002\n",
            "Epoch 14: loss improved from 6.77135 to 6.70017, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 314ms/step - loss: 6.7002 - lr: 0.0010\n",
            "Epoch 15/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.6175\n",
            "Epoch 15: loss improved from 6.70017 to 6.61748, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 285ms/step - loss: 6.6175 - lr: 0.0010\n",
            "Epoch 16/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.4908\n",
            "Epoch 16: loss improved from 6.61748 to 6.49078, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 297ms/step - loss: 6.4908 - lr: 0.0010\n",
            "Epoch 17/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.3152\n",
            "Epoch 17: loss improved from 6.49078 to 6.31520, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 301ms/step - loss: 6.3152 - lr: 0.0010\n",
            "Epoch 18/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 6.0838\n",
            "Epoch 18: loss improved from 6.31520 to 6.08378, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 289ms/step - loss: 6.0838 - lr: 0.0010\n",
            "Epoch 19/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 5.8767\n",
            "Epoch 19: loss improved from 6.08378 to 5.87667, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 289ms/step - loss: 5.8767 - lr: 0.0010\n",
            "Epoch 20/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 5.6791\n",
            "Epoch 20: loss improved from 5.87667 to 5.67913, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 303ms/step - loss: 5.6791 - lr: 0.0010\n",
            "Epoch 21/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 5.4876\n",
            "Epoch 21: loss improved from 5.67913 to 5.48758, saving model to nextword1.h5\n",
            "63/63 [==============================] - 23s 368ms/step - loss: 5.4876 - lr: 0.0010\n",
            "Epoch 22/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 5.3045\n",
            "Epoch 22: loss improved from 5.48758 to 5.30450, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 302ms/step - loss: 5.3045 - lr: 0.0010\n",
            "Epoch 23/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 5.1306\n",
            "Epoch 23: loss improved from 5.30450 to 5.13060, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 298ms/step - loss: 5.1306 - lr: 0.0010\n",
            "Epoch 24/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 4.9830\n",
            "Epoch 24: loss improved from 5.13060 to 4.98296, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 315ms/step - loss: 4.9830 - lr: 0.0010\n",
            "Epoch 25/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 4.8090\n",
            "Epoch 25: loss improved from 4.98296 to 4.80900, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 299ms/step - loss: 4.8090 - lr: 0.0010\n",
            "Epoch 26/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 4.6477\n",
            "Epoch 26: loss improved from 4.80900 to 4.64772, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 305ms/step - loss: 4.6477 - lr: 0.0010\n",
            "Epoch 27/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 4.4934\n",
            "Epoch 27: loss improved from 4.64772 to 4.49335, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 309ms/step - loss: 4.4934 - lr: 0.0010\n",
            "Epoch 28/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 4.3436\n",
            "Epoch 28: loss improved from 4.49335 to 4.34357, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 297ms/step - loss: 4.3436 - lr: 0.0010\n",
            "Epoch 29/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 4.1576\n",
            "Epoch 29: loss improved from 4.34357 to 4.15759, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 314ms/step - loss: 4.1576 - lr: 0.0010\n",
            "Epoch 30/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 4.0199\n",
            "Epoch 30: loss improved from 4.15759 to 4.01991, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 295ms/step - loss: 4.0199 - lr: 0.0010\n",
            "Epoch 31/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.8777\n",
            "Epoch 31: loss improved from 4.01991 to 3.87766, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 312ms/step - loss: 3.8777 - lr: 0.0010\n",
            "Epoch 32/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.7263\n",
            "Epoch 32: loss improved from 3.87766 to 3.72630, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 286ms/step - loss: 3.7263 - lr: 0.0010\n",
            "Epoch 33/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.5803\n",
            "Epoch 33: loss improved from 3.72630 to 3.58029, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 294ms/step - loss: 3.5803 - lr: 0.0010\n",
            "Epoch 34/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.4622\n",
            "Epoch 34: loss improved from 3.58029 to 3.46221, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 314ms/step - loss: 3.4622 - lr: 0.0010\n",
            "Epoch 35/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.3405\n",
            "Epoch 35: loss improved from 3.46221 to 3.34046, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 293ms/step - loss: 3.3405 - lr: 0.0010\n",
            "Epoch 36/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.2377\n",
            "Epoch 36: loss improved from 3.34046 to 3.23773, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 308ms/step - loss: 3.2377 - lr: 0.0010\n",
            "Epoch 37/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.1369\n",
            "Epoch 37: loss improved from 3.23773 to 3.13686, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 302ms/step - loss: 3.1369 - lr: 0.0010\n",
            "Epoch 38/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 3.0538\n",
            "Epoch 38: loss improved from 3.13686 to 3.05385, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 287ms/step - loss: 3.0538 - lr: 0.0010\n",
            "Epoch 39/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.9434\n",
            "Epoch 39: loss improved from 3.05385 to 2.94338, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 314ms/step - loss: 2.9434 - lr: 0.0010\n",
            "Epoch 40/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.8533\n",
            "Epoch 40: loss improved from 2.94338 to 2.85334, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 290ms/step - loss: 2.8533 - lr: 0.0010\n",
            "Epoch 41/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.7611\n",
            "Epoch 41: loss improved from 2.85334 to 2.76108, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 310ms/step - loss: 2.7611 - lr: 0.0010\n",
            "Epoch 42/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.6899\n",
            "Epoch 42: loss improved from 2.76108 to 2.68992, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 303ms/step - loss: 2.6899 - lr: 0.0010\n",
            "Epoch 43/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.6521\n",
            "Epoch 43: loss improved from 2.68992 to 2.65212, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 304ms/step - loss: 2.6521 - lr: 0.0010\n",
            "Epoch 44/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.6232\n",
            "Epoch 44: loss improved from 2.65212 to 2.62323, saving model to nextword1.h5\n",
            "63/63 [==============================] - 21s 328ms/step - loss: 2.6232 - lr: 0.0010\n",
            "Epoch 45/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.6046\n",
            "Epoch 45: loss improved from 2.62323 to 2.60457, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 311ms/step - loss: 2.6046 - lr: 0.0010\n",
            "Epoch 46/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.5013\n",
            "Epoch 46: loss improved from 2.60457 to 2.50135, saving model to nextword1.h5\n",
            "63/63 [==============================] - 21s 334ms/step - loss: 2.5013 - lr: 0.0010\n",
            "Epoch 47/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.4439\n",
            "Epoch 47: loss improved from 2.50135 to 2.44391, saving model to nextword1.h5\n",
            "63/63 [==============================] - 21s 327ms/step - loss: 2.4439 - lr: 0.0010\n",
            "Epoch 48/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.3804\n",
            "Epoch 48: loss improved from 2.44391 to 2.38037, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 324ms/step - loss: 2.3804 - lr: 0.0010\n",
            "Epoch 49/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.3169\n",
            "Epoch 49: loss improved from 2.38037 to 2.31687, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 303ms/step - loss: 2.3169 - lr: 0.0010\n",
            "Epoch 50/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.2510\n",
            "Epoch 50: loss improved from 2.31687 to 2.25105, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 319ms/step - loss: 2.2510 - lr: 0.0010\n",
            "Epoch 51/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.2207\n",
            "Epoch 51: loss improved from 2.25105 to 2.22074, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 294ms/step - loss: 2.2207 - lr: 0.0010\n",
            "Epoch 52/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.1556\n",
            "Epoch 52: loss improved from 2.22074 to 2.15556, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 2.1556 - lr: 0.0010\n",
            "Epoch 53/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.1351\n",
            "Epoch 53: loss improved from 2.15556 to 2.13508, saving model to nextword1.h5\n",
            "63/63 [==============================] - 22s 345ms/step - loss: 2.1351 - lr: 0.0010\n",
            "Epoch 54/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.0904\n",
            "Epoch 54: loss improved from 2.13508 to 2.09037, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 290ms/step - loss: 2.0904 - lr: 0.0010\n",
            "Epoch 55/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.0705\n",
            "Epoch 55: loss improved from 2.09037 to 2.07046, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 324ms/step - loss: 2.0705 - lr: 0.0010\n",
            "Epoch 56/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.0442\n",
            "Epoch 56: loss improved from 2.07046 to 2.04417, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 291ms/step - loss: 2.0442 - lr: 0.0010\n",
            "Epoch 57/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 2.0141\n",
            "Epoch 57: loss improved from 2.04417 to 2.01408, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 308ms/step - loss: 2.0141 - lr: 0.0010\n",
            "Epoch 58/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.9806\n",
            "Epoch 58: loss improved from 2.01408 to 1.98063, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 290ms/step - loss: 1.9806 - lr: 0.0010\n",
            "Epoch 59/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.9916\n",
            "Epoch 59: loss did not improve from 1.98063\n",
            "63/63 [==============================] - 17s 277ms/step - loss: 1.9916 - lr: 0.0010\n",
            "Epoch 60/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.9455\n",
            "Epoch 60: loss improved from 1.98063 to 1.94545, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 307ms/step - loss: 1.9455 - lr: 0.0010\n",
            "Epoch 61/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.9158\n",
            "Epoch 61: loss improved from 1.94545 to 1.91582, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 1.9158 - lr: 0.0010\n",
            "Epoch 62/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.8805\n",
            "Epoch 62: loss improved from 1.91582 to 1.88047, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 289ms/step - loss: 1.8805 - lr: 0.0010\n",
            "Epoch 63/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.8508\n",
            "Epoch 63: loss improved from 1.88047 to 1.85084, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 308ms/step - loss: 1.8508 - lr: 0.0010\n",
            "Epoch 64/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.8100\n",
            "Epoch 64: loss improved from 1.85084 to 1.80999, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 286ms/step - loss: 1.8100 - lr: 0.0010\n",
            "Epoch 65/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.7726\n",
            "Epoch 65: loss improved from 1.80999 to 1.77263, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 291ms/step - loss: 1.7726 - lr: 0.0010\n",
            "Epoch 66/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.7416\n",
            "Epoch 66: loss improved from 1.77263 to 1.74161, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 304ms/step - loss: 1.7416 - lr: 0.0010\n",
            "Epoch 67/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.7772\n",
            "Epoch 67: loss did not improve from 1.74161\n",
            "63/63 [==============================] - 18s 280ms/step - loss: 1.7772 - lr: 0.0010\n",
            "Epoch 68/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.7648\n",
            "Epoch 68: loss did not improve from 1.74161\n",
            "63/63 [==============================] - 18s 290ms/step - loss: 1.7648 - lr: 0.0010\n",
            "Epoch 69/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.7370\n",
            "Epoch 69: loss improved from 1.74161 to 1.73698, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 304ms/step - loss: 1.7370 - lr: 0.0010\n",
            "Epoch 70/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6935\n",
            "Epoch 70: loss improved from 1.73698 to 1.69353, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 286ms/step - loss: 1.6935 - lr: 0.0010\n",
            "Epoch 71/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6991\n",
            "Epoch 71: loss did not improve from 1.69353\n",
            "63/63 [==============================] - 19s 297ms/step - loss: 1.6991 - lr: 0.0010\n",
            "Epoch 72/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6881\n",
            "Epoch 72: loss improved from 1.69353 to 1.68815, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 1.6881 - lr: 0.0010\n",
            "Epoch 73/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6433\n",
            "Epoch 73: loss improved from 1.68815 to 1.64331, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 307ms/step - loss: 1.6433 - lr: 0.0010\n",
            "Epoch 74/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6173\n",
            "Epoch 74: loss improved from 1.64331 to 1.61731, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 305ms/step - loss: 1.6173 - lr: 0.0010\n",
            "Epoch 75/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6046\n",
            "Epoch 75: loss improved from 1.61731 to 1.60463, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 1.6046 - lr: 0.0010\n",
            "Epoch 76/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.5965\n",
            "Epoch 76: loss improved from 1.60463 to 1.59649, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 301ms/step - loss: 1.5965 - lr: 0.0010\n",
            "Epoch 77/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.6023\n",
            "Epoch 77: loss did not improve from 1.59649\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 1.6023 - lr: 0.0010\n",
            "Epoch 78/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.5893\n",
            "Epoch 78: loss improved from 1.59649 to 1.58927, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 1.5893 - lr: 0.0010\n",
            "Epoch 79/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.5814\n",
            "Epoch 79: loss improved from 1.58927 to 1.58141, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 312ms/step - loss: 1.5814 - lr: 0.0010\n",
            "Epoch 80/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.5441\n",
            "Epoch 80: loss improved from 1.58141 to 1.54406, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 283ms/step - loss: 1.5441 - lr: 0.0010\n",
            "Epoch 81/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.5347\n",
            "Epoch 81: loss improved from 1.54406 to 1.53472, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 1.5347 - lr: 0.0010\n",
            "Epoch 82/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.4952\n",
            "Epoch 82: loss improved from 1.53472 to 1.49524, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 302ms/step - loss: 1.4952 - lr: 0.0010\n",
            "Epoch 83/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.4696\n",
            "Epoch 83: loss improved from 1.49524 to 1.46956, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 1.4696 - lr: 0.0010\n",
            "Epoch 84/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.4867\n",
            "Epoch 84: loss did not improve from 1.46956\n",
            "63/63 [==============================] - 17s 270ms/step - loss: 1.4867 - lr: 0.0010\n",
            "Epoch 85/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.5052\n",
            "Epoch 85: loss did not improve from 1.46956\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 1.5052 - lr: 0.0010\n",
            "Epoch 86/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.4731\n",
            "Epoch 86: loss did not improve from 1.46956\n",
            "\n",
            "Epoch 86: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "63/63 [==============================] - 22s 348ms/step - loss: 1.4731 - lr: 0.0010\n",
            "Epoch 87/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.1156\n",
            "Epoch 87: loss improved from 1.46956 to 1.11560, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 286ms/step - loss: 1.1156 - lr: 2.0000e-04\n",
            "Epoch 88/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9728\n",
            "Epoch 88: loss improved from 1.11560 to 0.97276, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 297ms/step - loss: 0.9728 - lr: 2.0000e-04\n",
            "Epoch 89/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.9185\n",
            "Epoch 89: loss improved from 0.97276 to 0.91847, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 287ms/step - loss: 0.9185 - lr: 2.0000e-04\n",
            "Epoch 90/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8897\n",
            "Epoch 90: loss improved from 0.91847 to 0.88970, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 290ms/step - loss: 0.8897 - lr: 2.0000e-04\n",
            "Epoch 91/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8702\n",
            "Epoch 91: loss improved from 0.88970 to 0.87025, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 294ms/step - loss: 0.8702 - lr: 2.0000e-04\n",
            "Epoch 92/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8571\n",
            "Epoch 92: loss improved from 0.87025 to 0.85707, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 0.8571 - lr: 2.0000e-04\n",
            "Epoch 93/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8462\n",
            "Epoch 93: loss improved from 0.85707 to 0.84623, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 286ms/step - loss: 0.8462 - lr: 2.0000e-04\n",
            "Epoch 94/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8392\n",
            "Epoch 94: loss improved from 0.84623 to 0.83922, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 294ms/step - loss: 0.8392 - lr: 2.0000e-04\n",
            "Epoch 95/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8316\n",
            "Epoch 95: loss improved from 0.83922 to 0.83155, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 285ms/step - loss: 0.8316 - lr: 2.0000e-04\n",
            "Epoch 96/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8219\n",
            "Epoch 96: loss improved from 0.83155 to 0.82188, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 290ms/step - loss: 0.8219 - lr: 2.0000e-04\n",
            "Epoch 97/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8183\n",
            "Epoch 97: loss improved from 0.82188 to 0.81828, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 291ms/step - loss: 0.8183 - lr: 2.0000e-04\n",
            "Epoch 98/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8133\n",
            "Epoch 98: loss improved from 0.81828 to 0.81335, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 281ms/step - loss: 0.8133 - lr: 2.0000e-04\n",
            "Epoch 99/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8081\n",
            "Epoch 99: loss improved from 0.81335 to 0.80814, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 296ms/step - loss: 0.8081 - lr: 2.0000e-04\n",
            "Epoch 100/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8054\n",
            "Epoch 100: loss improved from 0.80814 to 0.80538, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 295ms/step - loss: 0.8054 - lr: 2.0000e-04\n",
            "Epoch 101/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8010\n",
            "Epoch 101: loss improved from 0.80538 to 0.80100, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 281ms/step - loss: 0.8010 - lr: 2.0000e-04\n",
            "Epoch 102/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7960\n",
            "Epoch 102: loss improved from 0.80100 to 0.79600, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 293ms/step - loss: 0.7960 - lr: 2.0000e-04\n",
            "Epoch 103/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7963\n",
            "Epoch 103: loss did not improve from 0.79600\n",
            "63/63 [==============================] - 18s 286ms/step - loss: 0.7963 - lr: 2.0000e-04\n",
            "Epoch 104/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7898\n",
            "Epoch 104: loss improved from 0.79600 to 0.78976, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 280ms/step - loss: 0.7898 - lr: 2.0000e-04\n",
            "Epoch 105/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7863\n",
            "Epoch 105: loss improved from 0.78976 to 0.78633, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 293ms/step - loss: 0.7863 - lr: 2.0000e-04\n",
            "Epoch 106/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7854\n",
            "Epoch 106: loss improved from 0.78633 to 0.78535, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 292ms/step - loss: 0.7854 - lr: 2.0000e-04\n",
            "Epoch 107/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7824\n",
            "Epoch 107: loss improved from 0.78535 to 0.78244, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 0.7824 - lr: 2.0000e-04\n",
            "Epoch 108/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7788\n",
            "Epoch 108: loss improved from 0.78244 to 0.77881, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 294ms/step - loss: 0.7788 - lr: 2.0000e-04\n",
            "Epoch 109/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7765\n",
            "Epoch 109: loss improved from 0.77881 to 0.77653, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 287ms/step - loss: 0.7765 - lr: 2.0000e-04\n",
            "Epoch 110/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7730\n",
            "Epoch 110: loss improved from 0.77653 to 0.77304, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 281ms/step - loss: 0.7730 - lr: 2.0000e-04\n",
            "Epoch 111/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7746\n",
            "Epoch 111: loss did not improve from 0.77304\n",
            "63/63 [==============================] - 17s 276ms/step - loss: 0.7746 - lr: 2.0000e-04\n",
            "Epoch 112/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7702\n",
            "Epoch 112: loss improved from 0.77304 to 0.77021, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 299ms/step - loss: 0.7702 - lr: 2.0000e-04\n",
            "Epoch 113/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7676\n",
            "Epoch 113: loss improved from 0.77021 to 0.76763, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 282ms/step - loss: 0.7676 - lr: 2.0000e-04\n",
            "Epoch 114/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7643\n",
            "Epoch 114: loss improved from 0.76763 to 0.76431, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 290ms/step - loss: 0.7643 - lr: 2.0000e-04\n",
            "Epoch 115/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7641\n",
            "Epoch 115: loss improved from 0.76431 to 0.76413, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 298ms/step - loss: 0.7641 - lr: 2.0000e-04\n",
            "Epoch 116/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7617\n",
            "Epoch 116: loss improved from 0.76413 to 0.76173, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 0.7617 - lr: 2.0000e-04\n",
            "Epoch 117/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7593\n",
            "Epoch 117: loss improved from 0.76173 to 0.75928, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 294ms/step - loss: 0.7593 - lr: 2.0000e-04\n",
            "Epoch 118/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7592\n",
            "Epoch 118: loss improved from 0.75928 to 0.75922, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 293ms/step - loss: 0.7592 - lr: 2.0000e-04\n",
            "Epoch 119/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7539\n",
            "Epoch 119: loss improved from 0.75922 to 0.75387, saving model to nextword1.h5\n",
            "63/63 [==============================] - 22s 355ms/step - loss: 0.7539 - lr: 2.0000e-04\n",
            "Epoch 120/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7544\n",
            "Epoch 120: loss did not improve from 0.75387\n",
            "63/63 [==============================] - 18s 294ms/step - loss: 0.7544 - lr: 2.0000e-04\n",
            "Epoch 121/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7500\n",
            "Epoch 121: loss improved from 0.75387 to 0.75003, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 291ms/step - loss: 0.7500 - lr: 2.0000e-04\n",
            "Epoch 122/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7497\n",
            "Epoch 122: loss improved from 0.75003 to 0.74973, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 289ms/step - loss: 0.7497 - lr: 2.0000e-04\n",
            "Epoch 123/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7495\n",
            "Epoch 123: loss improved from 0.74973 to 0.74952, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 308ms/step - loss: 0.7495 - lr: 2.0000e-04\n",
            "Epoch 124/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7457\n",
            "Epoch 124: loss improved from 0.74952 to 0.74575, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 287ms/step - loss: 0.7457 - lr: 2.0000e-04\n",
            "Epoch 125/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7417\n",
            "Epoch 125: loss improved from 0.74575 to 0.74167, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 293ms/step - loss: 0.7417 - lr: 2.0000e-04\n",
            "Epoch 126/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7414\n",
            "Epoch 126: loss improved from 0.74167 to 0.74141, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 303ms/step - loss: 0.7414 - lr: 2.0000e-04\n",
            "Epoch 127/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7437\n",
            "Epoch 127: loss did not improve from 0.74141\n",
            "63/63 [==============================] - 18s 279ms/step - loss: 0.7437 - lr: 2.0000e-04\n",
            "Epoch 128/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7383\n",
            "Epoch 128: loss improved from 0.74141 to 0.73833, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 289ms/step - loss: 0.7383 - lr: 2.0000e-04\n",
            "Epoch 129/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7400\n",
            "Epoch 129: loss did not improve from 0.73833\n",
            "63/63 [==============================] - 19s 299ms/step - loss: 0.7400 - lr: 2.0000e-04\n",
            "Epoch 130/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7396\n",
            "Epoch 130: loss did not improve from 0.73833\n",
            "63/63 [==============================] - 18s 280ms/step - loss: 0.7396 - lr: 2.0000e-04\n",
            "Epoch 131/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7363\n",
            "Epoch 131: loss improved from 0.73833 to 0.73628, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 296ms/step - loss: 0.7363 - lr: 2.0000e-04\n",
            "Epoch 132/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7355\n",
            "Epoch 132: loss improved from 0.73628 to 0.73550, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 307ms/step - loss: 0.7355 - lr: 2.0000e-04\n",
            "Epoch 133/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7313\n",
            "Epoch 133: loss improved from 0.73550 to 0.73132, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 0.7313 - lr: 2.0000e-04\n",
            "Epoch 134/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7348\n",
            "Epoch 134: loss did not improve from 0.73132\n",
            "63/63 [==============================] - 18s 293ms/step - loss: 0.7348 - lr: 2.0000e-04\n",
            "Epoch 135/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7307\n",
            "Epoch 135: loss improved from 0.73132 to 0.73071, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 295ms/step - loss: 0.7307 - lr: 2.0000e-04\n",
            "Epoch 136/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7314\n",
            "Epoch 136: loss did not improve from 0.73071\n",
            "63/63 [==============================] - 18s 281ms/step - loss: 0.7314 - lr: 2.0000e-04\n",
            "Epoch 137/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7308\n",
            "Epoch 137: loss did not improve from 0.73071\n",
            "63/63 [==============================] - 18s 291ms/step - loss: 0.7308 - lr: 2.0000e-04\n",
            "Epoch 138/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7282\n",
            "Epoch 138: loss improved from 0.73071 to 0.72820, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 298ms/step - loss: 0.7282 - lr: 2.0000e-04\n",
            "Epoch 139/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7237\n",
            "Epoch 139: loss improved from 0.72820 to 0.72368, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 296ms/step - loss: 0.7237 - lr: 2.0000e-04\n",
            "Epoch 140/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7230\n",
            "Epoch 140: loss improved from 0.72368 to 0.72302, saving model to nextword1.h5\n",
            "63/63 [==============================] - 20s 321ms/step - loss: 0.7230 - lr: 2.0000e-04\n",
            "Epoch 141/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7208\n",
            "Epoch 141: loss improved from 0.72302 to 0.72084, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 298ms/step - loss: 0.7208 - lr: 2.0000e-04\n",
            "Epoch 142/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7235\n",
            "Epoch 142: loss did not improve from 0.72084\n",
            "63/63 [==============================] - 20s 321ms/step - loss: 0.7235 - lr: 2.0000e-04\n",
            "Epoch 143/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7214\n",
            "Epoch 143: loss did not improve from 0.72084\n",
            "63/63 [==============================] - 18s 288ms/step - loss: 0.7214 - lr: 2.0000e-04\n",
            "Epoch 144/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7197\n",
            "Epoch 144: loss improved from 0.72084 to 0.71974, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 295ms/step - loss: 0.7197 - lr: 2.0000e-04\n",
            "Epoch 145/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7231\n",
            "Epoch 145: loss did not improve from 0.71974\n",
            "63/63 [==============================] - 19s 307ms/step - loss: 0.7231 - lr: 2.0000e-04\n",
            "Epoch 146/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7199\n",
            "Epoch 146: loss did not improve from 0.71974\n",
            "63/63 [==============================] - 18s 283ms/step - loss: 0.7199 - lr: 2.0000e-04\n",
            "Epoch 147/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7174\n",
            "Epoch 147: loss improved from 0.71974 to 0.71741, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 296ms/step - loss: 0.7174 - lr: 2.0000e-04\n",
            "Epoch 148/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7177\n",
            "Epoch 148: loss did not improve from 0.71741\n",
            "63/63 [==============================] - 19s 309ms/step - loss: 0.7177 - lr: 2.0000e-04\n",
            "Epoch 149/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7161\n",
            "Epoch 149: loss improved from 0.71741 to 0.71605, saving model to nextword1.h5\n",
            "63/63 [==============================] - 18s 294ms/step - loss: 0.7161 - lr: 2.0000e-04\n",
            "Epoch 150/150\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7130\n",
            "Epoch 150: loss improved from 0.71605 to 0.71300, saving model to nextword1.h5\n",
            "63/63 [==============================] - 19s 295ms/step - loss: 0.7130 - lr: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e8e544d39d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ox2k9Is90wEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}